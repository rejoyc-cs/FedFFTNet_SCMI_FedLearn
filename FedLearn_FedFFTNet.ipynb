{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fecd290-97e8-4b3d-955e-66b75dc6b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "from torchsummary import summary\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from torchvision import models\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import math\n",
    "import glob as glob\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7192358-5bb4-42e2-8a5c-6b765ffc560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)  # Only for single GPU\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Ensuring deterministic behavior in convolutional operations\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c022935-3c7a-4147-8a8f-47977769c67f",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d848b9f-3fbf-4081-b079-b05fafd43b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './patches'\n",
    "\n",
    "global_available=True #If not available, then make it false\n",
    "\n",
    "#Hyper-parameters\n",
    "batchsize=128\n",
    "ROUNDS = 100\n",
    "CLIENT_EPOCHS=1\n",
    "GLOBAL_EPOCHS=30\n",
    "learning_rate=0.001\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0') #Update according to your requirement\n",
    "\n",
    "#Extra printing param\n",
    "prompt=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442862cd-4261-4e45-9514-93107c1460f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "dataset = cwd.split('/')[-2]\n",
    "method = cwd.split('/')[-1]\n",
    "\n",
    "if global_available:\n",
    "    initial_datapath = os.path.join(root,'initial')\n",
    "    \n",
    "clients_path = os.path.join(root,'clients')\n",
    "test_datapath = os.path.join(root,'test')\n",
    "\n",
    "model_name = agg_algo+\"_\"+dataset+\"_\"+method+\".pth\"\n",
    "\n",
    "if global_available:\n",
    "    preclient_csv = 'preClientTrain_'+method+'.csv'\n",
    "    \n",
    "postclient_csv = 'postClientTrain_'+method+'.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ea4f7-0960-4013-9075-f084e09ac8ba",
   "metadata": {},
   "source": [
    "DATA LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2337d0-b8d4-4c6d-96f4-a86ed128b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = [\n",
    "   '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "   '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP','.mat',\n",
    "]\n",
    "\n",
    "def is_image_file(filename):\n",
    "   return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def find_classes(dir):\n",
    "   classes = os.listdir(dir)\n",
    "   classes.sort()\n",
    "   class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "   return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44315fe-dbf5-44d8-8741-138771d16b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(dir, class_to_idx):\n",
    "   images = []\n",
    "   for target in os.listdir(dir):\n",
    "       d = os.path.join(dir, target)\n",
    "       if not os.path.isdir(d):\n",
    "           continue\n",
    "\n",
    "       for filename in os.listdir(d):\n",
    "           if is_image_file(filename):\n",
    "               path = '{0}/{1}'.format(target, filename)\n",
    "               item = (path, class_to_idx[target])\n",
    "               images.append(item)\n",
    "\n",
    "   return images\n",
    "\n",
    "def default_loader(path):\n",
    "   return Image.open(path).convert('RGB')\n",
    "\n",
    "def mat_loader(path):\n",
    "   return scipy.io.loadmat(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f87ed1-cddb-4fed-bfa4-7cfc33cfcdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, class_to_idx = find_classes(test_datapath)\n",
    "dataset = make_dataset(test_datapath, class_to_idx)\n",
    "if prompt:\n",
    "    print(\"Dataset Structure(first 5):\\n\",dataset[:5])\n",
    "    print(\"\\nLength of Dataset: \",len(dataset))\n",
    "    print(\"\\nClass Mapping:\\n\",class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a5ffb-bfa4-483e-af15-08c4b0bcc3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderLoader(data.Dataset):\n",
    "    def __init__(self, root1, transform_1=None,\n",
    "                 target_transform=None,\n",
    "                 loader=default_loader, filename_return=False):\n",
    "        classes1, class_to_idx1 = find_classes(root1)\n",
    "        imgs1 = make_dataset(root1, class_to_idx1)\n",
    "\n",
    "        self.root1 = root1\n",
    "        self.imgs1 = imgs1\n",
    "        self.classes1 = classes1\n",
    "        self.class_to_idx1 = class_to_idx1\n",
    "        self.transform_1 = transform_1\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "\n",
    "        self.img_noise = None\n",
    "        self.img_rgb = None\n",
    "        self.filename_return = filename_return\n",
    "\n",
    "    def FFT(self):\n",
    "        img = self.img_rgb  # PIL image\n",
    "        img_np = np.array(img)  # Convert to NumPy array (H, W, C)\n",
    "\n",
    "        if img_np.ndim == 2:\n",
    "            img_np = img_np[:, :, np.newaxis]\n",
    "\n",
    "        fft_channels = []\n",
    "        for c in range(img_np.shape[2]):\n",
    "            channel = img_np[:, :, c]\n",
    "            f = np.fft.fft2(channel)\n",
    "            fshift = np.fft.fftshift(f)\n",
    "            magnitude_spectrum = 20 * np.log(np.abs(fshift) + 1e-8)  # avoid log(0)\n",
    "            fft_channels.append(magnitude_spectrum)\n",
    "\n",
    "        fft_np = np.stack(fft_channels, axis=0)  # Shape: (C, H, W)\n",
    "        self.img_noise = torch.tensor(fft_np, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path1, target1 = self.imgs1[index]\n",
    "        img1 = self.loader(os.path.join(self.root1, path1))\n",
    "        filename = Path(path1).stem\n",
    "\n",
    "        self.img_rgb = img1\n",
    "        self.FFT()\n",
    "\n",
    "        if self.transform_1 is not None:\n",
    "            img1 = self.transform_1(self.img_rgb)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target1 = self.target_transform(target1)\n",
    "\n",
    "        if self.filename_return:\n",
    "            return img1, self.img_noise, target1, filename\n",
    "\n",
    "        return img1, self.img_noise, target1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs1)\n",
    "\n",
    "\n",
    "# Define your transforms\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb53e5-0f13-4d30-b72a-0549fe1866c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if global_available:\n",
    "    initial_dataset = ImageFolderLoader(initial_datapath,transform_1=data_transforms)\n",
    "    \n",
    "    initial_loader = torch.utils.data.DataLoader(\n",
    "            initial_dataset, batch_size=batchsize,\n",
    "            shuffle=True, num_workers=8\n",
    "      )\n",
    "\n",
    "\n",
    "clients_loader = []\n",
    "clients = sorted(os.listdir(clients_path))\n",
    "for client in clients:\n",
    "    clt_path = os.path.join(clients_path, client)\n",
    "\n",
    "    client_dataset = ImageFolderLoader(clt_path,transform_1=data_transforms)\n",
    "\n",
    "    client_loader = torch.utils.data.DataLoader(\n",
    "        client_dataset, batch_size=batchsize,\n",
    "        shuffle=True, num_workers=8\n",
    "      )\n",
    "\n",
    "    clients_loader.append(client_loader)\n",
    "\n",
    "\n",
    "test_dataset = ImageFolderLoader(test_datapath,transform_1=data_transforms)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batchsize,\n",
    "        shuffle=True, num_workers=8\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f0d9f9-29a8-480f-88ca-95db65fab2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_client_images = 0\n",
    "clients = len(clients_loader)\n",
    "client_images = []\n",
    "for i, client_loader in enumerate(clients_loader):\n",
    "    num_images_client =len(client_loader.dataset)\n",
    "    client_images.append(num_images_client)\n",
    "    total_client_images += num_images_client\n",
    "    \n",
    "if prompt:\n",
    "    print(\"Total client images: \",total_client_images)\n",
    "    print(\"Images per client:\",client_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e95634-212a-440e-88f7-f1173ffc6f44",
   "metadata": {},
   "source": [
    "MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa253b-02ca-4f44-bcd8-b61abeba75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // reduction, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // reduction, in_planes, 1, bias=False)\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.shared(self.avg_pool(x))\n",
    "        max_out = self.shared(self.max_pool(x))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg, max_], dim=1)\n",
    "        return self.sigmoid(self.conv(x))\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, reduction=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ca = ChannelAttention(in_planes, reduction)\n",
    "        self.sa = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.ca(x)\n",
    "        x = x * self.sa(x)\n",
    "        return x\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Load pretrained MobileNetV2\n",
    "        mobilenet1 = models.mobilenet_v2(pretrained=True)\n",
    "        mobilenet2 = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "        # Extract only the feature extractor part (excluding classifier)\n",
    "        self.feature1 = mobilenet1.features  # [B, 1280, H/32, W/32]\n",
    "        self.feature2 = mobilenet2.features\n",
    "\n",
    "        # Final output of MobileNetV2 feature extractor is 1280 channels\n",
    "        self.cbam1 = CBAM(1280)\n",
    "        self.cbam2 = CBAM(1280)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Pool to [B, 1280, 1, 1]\n",
    "\n",
    "        self.fc1 = nn.Linear(1280 * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, len(class_to_idx))\n",
    "\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.feature1(x1)\n",
    "        x1 = self.cbam1(x1)\n",
    "        x1 = self.pool(x1)\n",
    "        x1 = x1.view(x1.size(0), -1)  # [B, 1280]\n",
    "\n",
    "        x2 = self.feature2(x2)\n",
    "        x2 = self.cbam2(x2)\n",
    "        x2 = self.pool(x2)\n",
    "        x2 = x2.view(x2.size(0), -1)  # [B, 1280]\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)  # [B, 2560]\n",
    "        x = self.fc1(x)                # [B, 512]\n",
    "        x = self.fc2(x)                # [B, num_classes]\n",
    "\n",
    "        out_fc = x\n",
    "        output = self.logsoftmax(x)\n",
    "        return output, out_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad28f75-0555-4ea7-bc2b-06a96606e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_categorical_cross_entropy(y_pred, y_true):\n",
    "    y_pred = torch.clamp(y_pred, 1e-9, 1 - 1e-9)\n",
    "    return -(y_true * torch.log(y_pred)).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbfd17f-619e-4be1-9a67-d7b699164fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelInit():\n",
    "    model = Net().to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b651a-bb96-4ac2-8274-406e59b6fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelInit()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizerGlobal = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "if prompt:\n",
    "    print(\"Device:- \",device)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de22f37d-5ed4-48d6-a87d-b10f1933a604",
   "metadata": {},
   "source": [
    "TRAINING AND TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bfaa38-e4fe-4949-8591-b7cc1ba0d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(model, test_loader, return_result=False):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    total_correct_test = 0\n",
    "    total_batches = len(test_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (imgs1,imgs2,labels1) in enumerate(test_loader):\n",
    "            img_org, mat_img, target = imgs1.to(device),imgs2.to(device), labels1.to(device)\n",
    "            output,_ = model(img_org,mat_img)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()  # Sum up batch loss\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct = (predicted == target).sum().item()\n",
    "            total_correct_test += correct\n",
    "\n",
    "            # Show progress\n",
    "            prompt=False\n",
    "            if prompt:\n",
    "                if batch_idx % 10 == 0 or batch_idx == total_batches - 1:  # Adjust frequency as needed\n",
    "                    print(f\"Processed {batch_idx + 1}/{total_batches} batches.\")\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = test_loss / total_batches\n",
    "    accuracy = 100.0 * total_correct_test / len(test_loader.dataset)\n",
    "    \n",
    "    # Print final results\n",
    "    print(f'\\nTest set: Average loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\\n')\n",
    "            \n",
    "    if return_result:\n",
    "        return {'accuracy': accuracy, 'loss': avg_loss}\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c07d5-c7fe-4ea9-9eba-ecc0e27e6a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(model, train_data, optimizer, valid_data=None, epochs=1, return_result=True, \n",
    "          overwrite_model=True, save_path='best_model.pth', csv_file=None, \n",
    "          global_model=None):\n",
    "    \n",
    "    train_loss_history = []\n",
    "    valid_loss_history = []\n",
    "    train_accuracy_history = []\n",
    "    valid_accuracy_history = []\n",
    "    best_accuracy = 0  # Track the best validation accuracy\n",
    "\n",
    "    # Check if CSV logging is enabled\n",
    "    if csv_file:\n",
    "        csv_exists = os.path.isfile(csv_file)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_correct_train = 0\n",
    "        total_batches = len(train_data)\n",
    "\n",
    "        for batch_idx, (imgs1,imgs2,labels1) in enumerate(train_data):\n",
    "            img_org, mat_img, target = imgs1.to(device),imgs2.to(device), labels1.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output,_ = model(img_org,mat_img)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate accuracy for the batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct = (predicted == target).sum().item()\n",
    "            total_correct_train += correct\n",
    "\n",
    "        # Calculate and store average training loss and accuracy for the epoch\n",
    "        avg_train_loss = total_train_loss / total_batches\n",
    "        train_accuracy = 100.0 * total_correct_train / len(train_data.dataset)\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "        train_accuracy_history.append(train_accuracy)\n",
    "        print(f\"\\nEpoch [{epoch + 1}/{epochs}] - Average Training Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # Optional validation step\n",
    "        avg_valid_loss = None\n",
    "        valid_accuracy = None\n",
    "        if valid_data is not None:\n",
    "            validation_results = Test(model, valid_data, return_result=True)\n",
    "            avg_valid_loss = validation_results['loss']\n",
    "            valid_accuracy = validation_results['accuracy']\n",
    "            valid_loss_history.append(avg_valid_loss)\n",
    "            valid_accuracy_history.append(valid_accuracy)\n",
    "\n",
    "            # Save model if validation accuracy improves\n",
    "            if valid_accuracy > best_accuracy:\n",
    "                best_accuracy = valid_accuracy\n",
    "                if overwrite_model:\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "                    print(f\"New best model saved with accuracy: {valid_accuracy:.2f}%\\n\")\n",
    "\n",
    "        # Write metrics to CSV if csv_file parameter is provided\n",
    "        if csv_file:\n",
    "            with open(csv_file, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                # Write header only if the file does not exist\n",
    "                if not csv_exists:\n",
    "                    writer.writerow(['Epoch', 'Train Loss', 'Train Accuracy', 'Valid Loss', 'Valid Accuracy'])\n",
    "                    csv_exists = True  # Set flag to avoid rewriting header in subsequent epochs\n",
    "\n",
    "                writer.writerow([\n",
    "                    epoch + 1,\n",
    "                    avg_train_loss,\n",
    "                    train_accuracy,\n",
    "                    avg_valid_loss if valid_data else None,\n",
    "                    valid_accuracy if valid_data else None\n",
    "                ])\n",
    "\n",
    "    # Return training and validation results if required\n",
    "    if return_result:\n",
    "        results = {\n",
    "            \"train_loss\": train_loss_history,\n",
    "            \"train_accuracy\": train_accuracy_history,\n",
    "            \"valid_loss\": valid_loss_history if valid_data else None,\n",
    "            \"valid_accuracy\": valid_accuracy_history if valid_data else None\n",
    "        }\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7be577-dee7-4c00-b5fc-32b1948952b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fedAvg(global_model, client_models, client_images=client_images, total_client_images=total_client_images):\n",
    "    # Initialize an empty state dictionary to accumulate the weighted weights\n",
    "    avg_state_dict = {key: torch.zeros_like(value) for key, value in global_model.state_dict().items()}\n",
    "\n",
    "    # Weighted sum of each client's model parameters\n",
    "    for i in range(len(client_models)):\n",
    "        client_state_dict = client_models[i].state_dict()\n",
    "        weight = client_images[i] / total_client_images\n",
    "        for key in avg_state_dict:\n",
    "            if avg_state_dict[key].dtype == torch.float32:  # Only perform weighted average for float tensors\n",
    "                avg_state_dict[key] += client_state_dict[key] * weight\n",
    "            else:\n",
    "                avg_state_dict[key] += client_state_dict[key]  # Direct addition for non-float types\n",
    "\n",
    "    # Load the averaged weights back into the global model\n",
    "    global_model.load_state_dict(avg_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07c83b-1e78-491b-9759-b4c88465a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clientTrain(global_model, clients_loader, rounds=1, client_epochs=1,method=\"FedAvg\", return_result=True, save_path='best_model.pth', csv_file=None):\n",
    "    acc = []\n",
    "    loss = []\n",
    "    best_acc = None\n",
    "\n",
    "    # Check if CSV logging is enabled\n",
    "    if csv_file:\n",
    "        csv_exists = os.path.isfile(csv_file)\n",
    "\n",
    "    for round in range(rounds):\n",
    "        if prompt:\n",
    "            print(\"Round: \", (round + 1))\n",
    "        client_models = []\n",
    "\n",
    "        for i in range(clients):\n",
    "            if prompt:\n",
    "                print(\"Client: \", (i + 1))\n",
    "            modelClient = modelInit()\n",
    "            optimizerClient = torch.optim.Adam(modelClient.parameters(), lr=learning_rate)\n",
    "            modelClient.load_state_dict(global_model.state_dict())\n",
    "\n",
    "            clientHistory = Train(modelClient, clients_loader[i],optimizer=optimizerClient, epochs=client_epochs, overwrite_model=False)\n",
    "    \n",
    "            client_models.append(modelClient)\n",
    "\n",
    "        fedAvg(global_model, client_models, client_images=client_images, total_client_images=total_client_images)\n",
    "    \n",
    "        # Validation on test data\n",
    "        result = Test(global_model, test_loader, return_result=True)\n",
    "        acc.append(result['accuracy'])\n",
    "        loss.append(result['loss'])\n",
    "\n",
    "        # Save best model\n",
    "        if best_acc is None or acc[-1] > best_acc:\n",
    "            best_acc = acc[-1]\n",
    "            torch.save(global_model.state_dict(), save_path)\n",
    "\n",
    "        # Write metrics to CSV if csv_file parameter is provided\n",
    "        if csv_file:\n",
    "            with open(csv_file, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                # Write header only if the file does not exist\n",
    "                if not csv_exists:\n",
    "                    writer.writerow(['Round', 'Validation Loss', 'Validation Accuracy'])\n",
    "                    csv_exists = True  # Update flag to prevent re-writing the header\n",
    "\n",
    "                writer.writerow([round + 1, result['loss'], result['accuracy']])\n",
    "\n",
    "    # Return validation loss and accuracy history if required\n",
    "    if return_result:\n",
    "        results = {\n",
    "            \"valid_loss\": loss,\n",
    "            \"valid_accuracy\": acc\n",
    "        }\n",
    "        return results\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9e578-edf0-49da-9e89-20ab44d1eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if global_available:\n",
    "    history = Train(model, epochs=GLOBAL_EPOCHS,optimizer = optimizerGlobal, train_data=initial_loader, valid_data=test_loader, save_path=model_name,csv_file=preclient_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ae96c0-4d2f-4231-9f35-2186c95fad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e7f37-3551-4c18-9387-e38b4312700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = clientTrain(model,clients_loader,client_epochs=CLIENT_EPOCHS,rounds=ROUNDS,save_path=model_name,csv_file=postclient_csv,method=agg_algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeed907-8436-40ef-90bc-f9e6fa78dfcc",
   "metadata": {},
   "source": [
    "FINDING PLA AND ILA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a471e70-ec65-45e4-9627-070e6126eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c0ccf0-2fa9-4abc-9d04-e8a541bbdc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ImageFolderLoader(\n",
    "        test_datapath,\n",
    "        data_transforms,\n",
    "        filename_return=True\n",
    "    )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=1,\n",
    "        shuffle=False, num_workers=12\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be0405-df33-46da-bc1f-860b25bc92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_level_accuracy(test_loader, model, result_folder='Results'):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    os.makedirs(result_folder, exist_ok=True)\n",
    "    \n",
    "    class_data = defaultdict(list)\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    progress_interval = max(len(test_loader) // 10, 1)  # Print progress every 10% of batches\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs1,imgs2, targets, filenames) in enumerate(test_loader):\n",
    "            img_org,mat_img, target = imgs1.to(device), imgs2.to(device), targets.to(device)\n",
    "            output, _ = model(img_org,mat_img)\n",
    "            predicted_class = torch.max(output, 1)[1]\n",
    "            target_class = target\n",
    "            \n",
    "            for j in range(img_org.size(0)):\n",
    "                filename = filenames[j]\n",
    "                entry = {\n",
    "                    'Filename': filename,\n",
    "                    'Target Class': target_class[j].item(),\n",
    "                    'Predicted Class': predicted_class[j].item()\n",
    "                }\n",
    "                class_data[target_class[j].item()].append(entry)\n",
    "\n",
    "                # Update accuracy tracking\n",
    "                if target_class[j].item() == predicted_class[j].item():\n",
    "                    total_correct += 1\n",
    "                total_samples += 1\n",
    "            \n",
    "            # Show progress at 10% intervals\n",
    "            if (i + 1) % progress_interval == 0 or i == len(test_loader) - 1:\n",
    "                print(f\"Progress Done (PLA): {((i + 1) / len(test_loader)) * 100:.1f}%\")\n",
    "\n",
    "    # Save predictions to class-specific CSV files\n",
    "    for target_class, entries in class_data.items():\n",
    "        df = pd.DataFrame(entries)\n",
    "        df.to_csv(os.path.join(result_folder, f'class_{target_class}.csv'), index=False)\n",
    "\n",
    "    # Calculate and print patch level accuracy\n",
    "    patch_level_accuracy = (total_correct / total_samples) * 100 if total_samples > 0 else 0\n",
    "    print(f\"Patch Level Accuracy: {patch_level_accuracy:.2f}%\")\n",
    "    \n",
    "    # Flatten predicted data for further processing\n",
    "    predicted_data = []\n",
    "    for entries in class_data.values():\n",
    "        for entry in entries:\n",
    "            predicted_data.append((entry['Filename'], entry['Target Class'], entry['Predicted Class']))\n",
    "    \n",
    "    return patch_level_accuracy,predicted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba3895-eb3b-4959-b090-063ff356bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_level_predictions(directory):\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(directory) if f.startswith(\"class_\") and f.endswith(\".csv\")]\n",
    "    total_images = 0\n",
    "    correct_images = 0\n",
    "\n",
    "    predictions = []  # List to store image-level predictions\n",
    "\n",
    "    for file in csv_files:\n",
    "        result_dict = {}\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        target_class = None\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            filename = row.iloc[0]  # First column\n",
    "            target_class = row.iloc[1]  # Second column (true class)\n",
    "            predicted_class = row.iloc[2]  # Third column (predicted class)\n",
    "\n",
    "            A_value, _ = filename.rsplit(\"_\", 1)\n",
    "            \n",
    "            if A_value not in result_dict:\n",
    "                result_dict[A_value] = []\n",
    "            \n",
    "            result_dict[A_value].append(predicted_class)\n",
    "\n",
    "        total_images += len(result_dict)\n",
    "\n",
    "        for key, values in result_dict.items():\n",
    "            voted_class = Counter(values).most_common(1)[0][0]\n",
    "            if voted_class == target_class:\n",
    "                correct_images += 1\n",
    "\n",
    "            predictions.append([key, target_class, voted_class])\n",
    "\n",
    "    output_file = os.path.join(directory, \"Image Level Prediction.csv\")\n",
    "    df_predictions = pd.DataFrame(predictions, columns=[\"Filename\", \"Target\", \"Predictions\"])\n",
    "    df_predictions.to_csv(output_file, index=False)\n",
    "\n",
    "    ila = (correct_images / total_images)*100\n",
    "    return ila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b6203a-64fd-4ac5-a89b-295d2578bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder='Results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b23a9e-d665-4045-8901-caa4418beda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pla,predicted_data = patch_level_accuracy(test_loader, model, result_folder=result_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066982b8-6394-41b8-897d-97bbd6388787",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2cfa85-f461-4c90-b497-cf37f46ad377",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = './'+result_folder\n",
    "ila = image_level_predictions(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08c67e5-42af-40fb-8308-bd216db1d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ila)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d131c-77a0-47ad-861d-caba0a5e9795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
